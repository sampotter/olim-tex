\documentclass[eikonal.tex]{subfiles}

\begin{document}

\section{Quadrature Rules}

In this section, we discuss how to compute updates. For each update,
we identify the updated point with the origin. We assume our method is
operating on a uniform grid with lattice constant $h$. The vectors
$\set{p_0, \hdots, p_d}$ of the previous section are the vectors of
the simplex corresponding to the current update. We assume these
vectors are scaled so that, if $\hat{p}$ denotes the update grid
point, then the points which lie on the (approximate) front of the
solution are $\hat{p} + h p_i \in \R^n$ for $i = 0, \hdots, d$. This
is useful because on a uniform grid, it will be convenient for us to
work with vectors $p_i$ whose components satisfy
$(p_i)_j \in \set{0, -1, 1}$ for $j = 1, \hdots, n$. In particular,
many of the quantities that we will encounter become integer-valued
and simpler to compute digitally (e.g., the dot product between two
such vectors can be computed rapidly using bitwise operations).

Each simplex update approximately minimize a line integral
corresponding to a trial evolution of a characteristic curve of the
numerical solution. We identify the current grid point with the origin
and denote the trial value of the numerical solution at this point by
$\hat{u}$. In general, we use a hat to denote quantities for the
update point; e.g., below, $\hat{s} = s(\hat{p})$. Then, the goal is to
approximate:
\begin{align}\label{eq:hat-u-variational}
  \hat{u} = \min_{\lambda \in \Delta^m} \set{u_\lambda + h \int_{[p_\lambda, 0]} s(\gamma(t))dt}.
\end{align}
using a quadrature rule, where $\gamma$ is an arc length
parametrization of
$[p_\lambda, 0] = \operatorname{conv}(\set{0, p_\lambda})$. We
consider two approximations to \cref{eq:hat-u-variational}: the
difference between the two pertains to the way we incorporate the
speed function $s$. Let $\theta \in [0, 1]$ and define
$l_\lambda = \norm{p_\lambda}_2$. Then, we define:
\begin{align*}
  F_0(\lambda; \theta) &:= u_\lambda + h s_\theta l_\lambda = u_\lambda + h \squareb{(1-\theta)\hat{s} + \frac{\theta}{d + 1} \sum_{i=0}^d s_i} l_\lambda, \\
  F_1(\lambda; \theta) &:= u_\lambda + h s_\theta(\lambda) l_\lambda = u_\lambda + h \squareb{(1-\theta)\hat{s} + \theta s_\lambda} l_\lambda.
\end{align*}
We will primarily concern ourselves with $F_0$ and $F_1$ for
$\theta = 0$ and $\theta = \nicefrac{1}{2}$. For $\theta = 0$, we have
$F_0 = F_1$, so we define $\Frhr = F_0 = F_1$. On the other hand,
$F_0(\lambda;\nicefrac{1}{2}) \neq F_1(\lambda;\nicefrac{1}{2})$
unless $s \equiv 1$, so we write
$\Fmpzero(\lambda) = F_0(\lambda; \nicefrac{1}{2})$ and
$\Fmpone(\lambda) = F_1(\lambda; \nicefrac{1}{2})$, where ``mp''
stands for ``midpoint''.

To compute $\hat{u}$, we will need access to the gradient and Hessian
of $F_0$ and $F_1$. These quantities are easy to compute, but we have
found a particular form for them to be appropriate. We compute these
quantities here.

\begin{lemma}\label{lemma:F0-grad-and-Hess}
  The gradient and Hessian of $F_0(\lambda; \theta)$ satisfy:
  \begin{align*}
    \nabla_\lambda F_0(\lambda; \theta) = \delta u + \frac{s_\theta h}{l_\lambda} \delta P^\top p_\lambda, \qquad
    \nabla^2_\lambda F_0(\lambda; \theta) = \frac{s_\theta h}{l_\lambda} \delta P^\top \calP^\perp_{p_\lambda} \delta P,
  \end{align*}
  where $\calP_{p_\lambda}$ denotes the orthogonal projector onto
  $\operatorname{span}(p_\lambda)$ and $\calP_{p_\lambda}^\perp$ is
  the orthogonal projector onto
  $\operatorname{span}(p_\lambda)^\perp$.
\end{lemma}

\begin{proof}
  For the gradient, we have:
  \begin{align*}
    \nabla_\lambda F_0(\lambda; \theta) = \delta u + \frac{s_\theta h}{2 l_\lambda} \nabla_\lambda p_\lambda^\top p_\lambda = \delta u + \frac{s_\theta h}{l_\lambda} \delta P^\top p_\lambda,
  \end{align*}
  since
  $\nabla_\lambda p_\lambda^\top p_\lambda = 2 \delta P^\top
  p_\lambda$. For the Hessian:
  \begin{align*}
    \nabla^2_\lambda F_0(\lambda; \theta) &= \nabla_\lambda \parens{\frac{s_\theta h}{l_\lambda} p_\lambda^\top \delta P} = s_\theta h \parens{\nabla_\lambda \frac{1}{l_\lambda} p_\lambda^\top \delta P + \frac{1}{l_\lambda} \nabla_\lambda p_\lambda^\top \delta P} \\
    &= \frac{s_\theta h}{l_\lambda} \parens{\delta P^\top \delta P - \frac{\delta P^\top p_\lambda p_\lambda^\top \delta P}{p_\lambda^\top p_\lambda}} = \frac{s_\theta h}{l_\lambda} \delta P^\top \parens{I - \frac{p_\lambda p_\lambda^\top}{p_\lambda^\top p_\lambda}} \delta P,
  \end{align*}
  from which the result follows.
\end{proof}

\begin{lemma}
  The gradient and Hessian of $F_1(\lambda; \theta)$ satisfy:
  \begin{align*}
    \nabla_\lambda F_1(\lambda; \theta) &= \delta u + \frac{h}{l_\lambda} \parens{\theta p_\lambda \delta s^\top + s_\theta(\lambda) \delta P}^\top p_\lambda, \\
    \nabla_\lambda^2 F_1(\lambda; \theta) &= \frac{h}{l_\lambda} \parens{\theta \parens{\delta P^\top p_\lambda \delta s^\top + \delta s p_\lambda^\top \delta P} + s_\theta(\lambda) \delta P^\top \calP^\perp_{p_\lambda} \delta P}.
  \end{align*}
\end{lemma}

\begin{proof}
  Since $F_1(\lambda; \theta) = u_\lambda + h s_\theta(\lambda) l_\lambda$, for the gradient we have:
  \begin{align*}
    \nabla_\lambda F_1(\lambda; \theta) = \delta u + h \parens{\theta l_\lambda \delta s + \frac{s_\theta(\lambda)}{2l_\lambda} \nabla_\lambda p_\lambda^\top p_\lambda} = \delta u + \frac{h}{l_\lambda} \parens{\theta p_\lambda^\top p_\lambda \delta s + s_\theta \delta P^\top p_\lambda},
  \end{align*}
  and for the Hessian:
  \begin{align*}
    \nabla_\lambda^2 F_1(\lambda; \theta) &= \frac{h}{2 l_\lambda} \parens{\theta \parens{\nabla_\lambda p_\lambda^\top p_\lambda \delta s^\top + \delta s {(\nabla_\lambda p_\lambda^\top p_\lambda)}^\top} + s_\theta(\lambda) \parens{\frac{1}{2 p_\lambda^\top p_\lambda} \nabla_\lambda p_\lambda^\top p_\lambda {(\nabla_\lambda p_\lambda^\top p_\lambda)}^\top - \nabla^2_\lambda p_\lambda^\top p_\lambda}}.
  \end{align*}
  Simplfying this gives us the result.
\end{proof}

\noindent \textbf{TODO}: \emph{as an aside, we could use Woodbury or
  Sherman-Morrison to write $\nabla^2 F_1^{-1}$ in terms of
  $\nabla^2 F_0^{-1}$. If we came up with a simple way to compute (or
  precompute and cache) $\nabla^2 F_0^{-1}$, then we might have access
  to a cheap Newton's method for both $F_0$ and $F_1$. If all we have
  to do is precompute a QR decomposition for each $\delta P$, and the
  resulting inverse is correct, the cost of inversion would be
  $O(nd + d^2) = O(n^2)$ (since $d \leq n$), where $n$ is the
  dimension of the ambient space and $d$ is the number of vectors
  $p_i$.}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
