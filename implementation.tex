\documentclass[eikonal.tex]{subfiles}

\begin{document}
  
\section{Implementation of the ordered line integral
  method}\label{sec:implementation}

\begin{figure}
  \centering
  \includegraphics{neighborhoods.eps}
  \caption{Ordered line integral method neighborhoods in 2D and 3D:
    \texttt{olim4} and \texttt{olim8} are 2D solvers and the rest are
    3D solvers. The color coding of tetrahedron updates is the same
    for this figure and \cref{fig:octant-numbering}
    below.}\label{fig:neighborhoods}%
  \vspace{-1.5em}
  \includegraphics{simplex-groups.eps}
  \caption{Numbering scheme for an update octant. In this diagram,
    $\hat{p}$ is being updated. The diagonally opposite node is the
    sixth (last) node, with the other six nodes numbered 0--5
    cyclically.}\label{fig:octant-numbering}
  {
    \footnotesize
    \begin{tabular}{c|cccccc|cccccc|cccccc|cc}
      0 & $\groupmarker$ & & & & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ & $\groupmarker$ & \\
      1 & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & & & $\groupmarker$ \\
      2 & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & & & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & $\groupmarker$ & \\
      3 & & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & & & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & $\groupmarker$ \\
      4 & & & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & \\
      5 & & & & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & & & $\groupmarker$ & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ & $\groupmarker$ & & $\groupmarker$ \\
      \multicolumn{1}{c}{} & \multicolumn{6}{c}{I} & \multicolumn{6}{c}{II} & \multicolumn{6}{c}{III} & \multicolumn{2}{c}{IV}
    \end{tabular}%
    \vspace{1em}
    \begin{tabular}{c|cccccc|cccccc|ccc}
      0 & $\groupmarker$ & & & & & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & & \\
      1 & $\groupmarker$ & $\groupmarker$ & & & & & & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & \\
      2 & & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & & & & & & $\groupmarker$ \\
      3 & & & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ & & \\
      4 & & & & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ & \\
      5 & & & & & $\groupmarker$ & $\groupmarker$ & & & & $\groupmarker$ & & $\groupmarker$ & & & $\groupmarker$ \\
      6 & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ & $\groupmarker$ \\
      \multicolumn{1}{c}{} & \multicolumn{6}{c}{V} & \multicolumn{6}{c}{VI} & \multicolumn{3}{c}{VII}
    \end{tabular}%
    \vspace{-0.5em}
  }
  \caption{These tables should be scanned columnwise: each column of
    dots indicates a different tetrahedron. Note that the tetrahedra
    (0, 1, 2), (2, 3, 4), and (4, 5, 0) in group I are degenerate and
    can be omitted; likewise, the tetrahedra in group VII are all
    degenerate and can do not form a useful
    group.}\label{fig:tetrahedra-groups}
\end{figure}

In this section, we describe our ``top-down'' and ``bottom-up''
algorithms. We emphasize the 3D solver, since in 2D, the distinction
between the two is less important. Each algorithm reduces the number
of updates that are done without degrading solution accuracy by using
an efficient enumeration or search of the neighboring simplexes. The
difference between the algorithms is in how this is done.

In \cref{ssec:simplex-enumeration}, we start by showing how to
enumerate update tetrahedra and put them into separate groups of
congruent tetrahedra. By selecting and only performing updates from
these groups, we obtain top-down algorithms with stencils of different
sizes. Our numerical tests (\cref{sec:numerical-results}) will show
how neighborhoods of different sizes lead to different patterns of
directional coverage, which can significantly affect the error. We
also discuss the update gaps attainable using these tetrahedron groups
in \cref{ssec:update-gaps}.

Following this, we describe our bottom-up algorithm in
\cref{ssec:bottom-up-search}, which involves first finding the minimal
update of smallest dimension ($d = 0$, a line update), then finding
the minimal update of the next highest dimension ($d = 1$, a triangle
update) which is incident to the original update, and so on. In 3D,
this means finding the minimal line update, doing neighboring triangle
updates which contain the minimal line update, and then neighboring
tetrahedron updates which contain the minimal triangle update. We can
think of this algorithm as a fast search for the first arrival
characteristic.

To minimize the number of updates that are done, it is important to
take advantage of the structure of the underlying constrained
optimization problems that are being solved in order to skip
unnecessary lower or higher dimensional updates. We describe this
procedure in \cref{ssec:algorithms-and-skipping}. How this is done
varies depending on the choice of quadrature rule (\texttt{mp0},
\texttt{mp1}, or \texttt{rhr}) and type of algorithm (top-down or
bottom-up).

\subsection{Simplex enumeration for the top-down
  algorithm}\label{ssec:simplex-enumeration}

When a node has just been taken off the heap and is newly
\texttt{valid} (\cref{enum:get-node}), an isotropic solver must do
updates involving, at the very least, the node's $2n$ cardinal
neighbors. It is also possible to use larger neighborhoods to improve
the accuracy of the result. Doing so does not improve the order of
convergence of the solver, but can still significantly improve the
accuracy of the solution. For all of the solvers considered in this
paper, in 3D, we only ever consider neighborhoods with at most 26
neighbors.

For the top-down solver, we first simplify things by treating a node's
neighboring octants separately. That is, we iterate over each octant
and do all updates that lie inside that octant before moving onto the
next. To do this, we enumerate all update tetrahedra with vertices
$p \in \{0, 1\}^3$ in a symmetric fashion. Since we assume our update
tetrahedra have been translated so that $\hat{p} = 0 \in \mathbb{Z}^n$
(see \cref{ssec:notation}), this means enumerating
${7 \choose 3} = 35$ choices of vertices. Some choices lead to
degenerate tetrehedra (i.e., such that $p_0, p_1, p_2$ aren't linearly
independent), so the number of nondegenerate update tetrahedra is
smaller. Since we iterate octant by octant, and since the number of
tetrahedra is relatively small, it is reasonable to write out the
update procedure as straight-line code, which is what we do in our own
implementation.

We enumerate the tetrahedra in a type of ``shift-order'' (see, e.g.,
\cite{arndt2010matters})---that is, we start with an unseen bit
pattern, and group this pattern together with all of its shifts (with
rotation). This groups the tetrahedra into sets that are rotationally
symmetric about the diagonal of the octant. In our implementation, we
conditionally compile different groups so that no unnecessary
branching is done. This is done using the template feature of
C++~\cite{stroustrup2013c++}. Example stencils for the versions of
\texttt{olim6}, \texttt{olim18}, and \texttt{olim26} that are used for
our numerical test are shown in \cref{fig:neighborhoods}. The
enumerated tetrahedron groups are shown in
\cref{fig:octant-numbering,fig:tetrahedra-groups}.

\subsection{Update gaps for tetrahedron
  groups}\label{ssec:update-gaps}

As an aside, if we apply \cref{thm:causality} to the tetrahedron
groups enumerated in
\cref{fig:octant-numbering,fig:tetrahedra-groups}, we get the
following update gaps (ignoring the $s^\theta h$ factor):
\vspace{0.5em}
\begin{center}
  \begin{tabular}{lc|lc|lc|lc}
    Group I & $1/\sqrt{2}$ & Group II & $1/\sqrt{2}$ & Group III & $1/\sqrt{2}$ & Group IVa & 0 \\
    \midrule
    Group V & $1/\sqrt{3}$ & Group VIa & 0 & Group VIb & $\boldsymbol{2/\sqrt{3}}$ & Group IVb & $1/\sqrt{2}$
  \end{tabular}
\end{center}
\vspace{0.5em} The idea of the update gap is first explored in
Tsitsiklis's original paper~\cite{tsitsiklis1995efficient}; in this
work, the fact that Group IVa has no update gap and that the update
gap of Group V is $1/\sqrt{3}$ is noted and an $O(N)$ algorithm based
on Dial's algorithm is presented using Group V for the update
tetrahedra. This same observation is made in a more recent paper
explicitly detailing a method based on Dial's
algorithm~\cite{kim2001calo}. A method based on a combination of
tetrahedra groups will have as its update gap the minima of each of
the individual groups' update gaps. We note here that a solver based
on a combination of Groups I and VIb has a larger update gap than a
solver based on Group V (around 1.22 times larger). This should have a
positive impact on the performance of any parallel Dijkstra-like
method.

\subsection{The search procedure used by the bottom-up
  algorithm}\label{ssec:bottom-up-search}

Another approach is to take advantage of the fact that lower
dimensional updates provide some information about the likely
direction of arrival of the first arrival time characteristic. For
instance, if we know where the minimum line update is, then the
characteristic is likely close by.

To this end, we start with the minimum line update, enumerate
neighboring vertices and perform the corresponding triangle updates,
then enumerate vertices which are sufficiently close to the minimum
triangle update, finally doing any relevant tetrahedron updates. While
the top-down algorithm is parametrized by the choice of tetrahedron
groups to include, the bottom-up algorithm can instead be parametrized
by the type of norm used when searching for neighboring vertices as
well as the permitted distance. For example, in 3D, if $p_0$ is the
vertex for the minimum line update, then $p_1$ must satisfy
$\norm{p_1 - p_0}_{q_1} \leq d_1$, where $q = 1, 2, \infty$ and
$d_1 = 1, 2, \hdots$. Likewise, when searching for tetrahedron
updates, if $p_2$ is the candidate vertex, then
$\norm{p_2 - p_1}_{q_2} \leq d_2$ and $\norm{p_2 - p_0}_q \leq d_2$
are required to hold simultaneously. For our numerical tests, we set
$q_1 = 1 = q_2$, $d_1 = 1$ and $d_2 = 2$. This setup is depicted in
\cref{fig:hu-neighborhoods}. We found this choice of parameters to be
a good compromise between speed and accuracy. Using larger
neighborhoods does lead to a more accurate solution, but may cause the
solver to run more slowly.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{hu-neighborhoods.eps}
  \caption{The three types of neighborhoods for the bottom-up
    algorithm with $q_1 = 1 = q_2$, $d_1 = 1$, and $d_2 = 2$. The
    yellow and blue regions indicate where triangle and tetrahedron
    updates may be performed, respectively. For instance, with $p_0$
    the minimizing line update vertex, candiates for $p_1$ consist of
    the yellow nodes: triangle updates involving these candidates and
    $p_0$ will be performed. Once a yellow node ($p_1$) has been
    selected, tetrahedron updates involving the neighboring blue nodes
    (candidates for $p_2$) will be performed. Note that the updates
    performed correspond roughly to a combination of groups I, V, VIa,
    and VIb.}\label{fig:hu-neighborhoods}
\end{figure}

\subsection{Minimization algorithms and skipping
  updates}\label{ssec:algorithms-and-skipping}

Before presenting the top-down and bottom-up algorithms, we go over
our approach to performing updates and skipping updates (when
possible).

Performing an update is the same as solving
\cref{eq:constrained-minimization} for fixed problem data. When
$F_i = F_0$, we can use \cref{thm:f0-exact} or \cref{thm:equivalence}
to compute $\lambda_0^*$. In this case, $\lambda_0^*$ may lie outside
$\Delta^d$. On the other hand, if $F_i = F_1$, we need to use an
algorithm that can solve the constrained optimization problem defined
by \cref{eq:constrained-minimization}. Our approach has been to use
sequential quadratic programming (SQP), although there are many other
options~\cite{bertsekas1999nonlinear,nocedal2006numerical}.

It is possible to skip updates in the course of running our update
algorithms, and indeed, their performance hinges on this happening. We
skip updates in three different ways.

The first is simple: if we do a higher-dimensional update (say, a
tetrahedron update) using a constrained solver, then we can rule out
all incident lower-dimensional updates (three triangle and three line
updates). The second is related: if we instead perform a
higher-dimensional update using an unconstrained solver, then
depending on where the optimum $\lambda_0^*$ lies, we can skip some
lower-dimensional updates. The idea is simple: since $F_0$ is strictly
convex, if we consider a straight line starting at $\lambda_0^*$ and
extending in some direction, then $f$ restricted to that line can only
increase as we move away from $\lambda_0^*$. Hence, for a tetrahedron
update, if $\lambda_0^* \notin \Delta^2$, then we can skip all updates
which are not ``visible'' (in parameter space) from
$\lambda_0^*$. This is illustrated in the \cref{fig:skip-zones}.

\begin{figure}
  \centering
  \includegraphics{skip-zones.eps}
  \caption{For $d = 2$, when minimizing $F_0$ using
    \cref{thm:f0-exact}, if $\lambda^*_0 \notin \Delta^2$, depending
    on where $\lambda_0^*$ lies, it is possible to skip one or two
    triangle updates. On the other hand, if
    $\lambda_0^* \in \Delta^2$, all three triangle updates can be
    skipped.}\label{fig:skip-zones}
\end{figure}

The first two approaches to skipping updates skip incident
lower-dimensional updates. It is also possible to skip incident
higher-dimensional updates. For example, if we do the three triangle
updates on the boundary of a tetrahedron update, we can use
Karush-Kuhn-Tucker (KKT) theory to determine if the minimizer on the
boundary is also a global minimizer for the tetrahedron update. Let
$L(\lambda, \mu) = F_i(\lambda) + (A\lambda - b)^\top \mu$ be the
Lagrangian function, where $\mu \in \mathbb{R}^{d + 1}$ is the vector
of Lagrange multipliers. Since $F_0$ is strictly convex and since we
assume $h$ is small enough for $F_1$ to be strictly convex, if
$\lambda^*$ lies on the boundary of $\Delta^d$, we only need to check
that the optimum Lagrange multipliers $\mu^*$ are dual feasible; i.e.,
whether
$\mu^* \geq 0$~\cite{bertsekas1999nonlinear,nocedal2006numerical}. For
$\lambda^* \in \partial \Delta^d$, letting
$\mathcal{I} = \set{i : (A\lambda - b)_i = 0}$ be the set of active
constraints' indices, stationarity requires:
\begin{equation}\label{eq:stationarity}
  A^\top_{\mathcal{I}} \mu_{\mathcal{I}}^* = \nabla F_i(\lambda).
\end{equation}
For a tetrahedron update in 3D, $A \in \mathbb{R}^{3 \times 2}$ and
$|\mathcal{I}| \leq 2$ (not all three constraints be active
simultaneously). In particular, if $i \notin \mathcal{I}$, then
$\mu_i^* = 0$, and otherwise, $\mu_i^*$ can be computed easily from
\cref{eq:stationarity}. Once the full vector of Lagrange multipliers
has been computed, if $\mu^* \geq 0$, then the update may be skipped.

\subsection{The top-down and bottom-up algorithms}

To describe our top-down algorithm, we define:
\begin{equation}\label{eq:calU}
  \calU_d = \set{(p_0, \hdots, p_d): p_i\texttt{.state}=\texttt{valid} \mbox{ for } i = 0, \hdots, d \mbox{ and } (p_0, \hdots, p_d) \mbox{ is in an update group}}
\end{equation}
for $d = 0, \hdots, n - 1$. For each update, this set collects all of
the possible simplex updates of each dimension: i.e., updates which
both belong to a group as defined in \cref{ssec:simplex-enumeration}
and are \texttt{valid}. Each set $\calU_d$ is initialized according to
\cref{eq:calU} but is modified as the algorithm runs and as
lower-dimensional updates are skipped (removed from $\calU_d$).

\begin{algorithm}[H]
  \caption{The top-down hierarchical algorithm for computing
    $U(\hat{p})$ (\cref{enum:update-U} of
    \cref{alg:dijkstra-like}).}\label{alg:top-down}
  \begin{enumerate}[nolistsep]
  \item Set $\hat{U} \gets \infty$.
  \item Initialize $\calC_d$ according to \cref{eq:calU} for each
    $d = 0, \hdots, n - 1$.
  \item For $d = n - 1$ down to $0$:
    \begin{enumerate}
    \item For each $(p_0, \hdots, p_d) \in \calC_d$:
      \begin{enumerate}
      \item If $F_i = F_0$:
        \begin{enumerate}
        \item Compute $U$ for $(p_0, \hdots, p_{d})$ using
          \cref{thm:f0-exact} or \cref{thm:equivalence}.
        \item Remove updates from $\calC_0, \hdots, \calC_{d-1}$
          according to their visibility (see \cref{fig:skip-zones}).
        \end{enumerate}
      \item Otherwise, if $F_i = F_1$:
        \begin{enumerate}
        \item Compute $U$ using a constrained minimization algorithm
          (SQP in our case).
        \item Remove all incident lower-dimensional updates from
          $\calC_0, \hdots, \calC_{d-1}$.
        \end{enumerate}
      \item Set $\hat{U} = \min(\hat{U}, U)$.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{algorithm}

On the other hand, the bottom-up algorithm builds up each update
$(p_0, \hdots, p_d)$ one vector at a time, by searching for adjacent
minimizing updates of higher dimension.

\begin{algorithm}[H]
  \caption{The bottom-up hierarchical algorithm for computing
    $U(\hat{p})$ (\cref{enum:update-U} of
    \cref{alg:dijkstra-like}).}\label{alg:bottom-up}
  \begin{enumerate}[nolistsep]
  \item Set $\hat{U} \gets \infty$.
  \item Find $p_0 \in \neib(\hat{p})$ such that $p_0$\texttt{.state}
    $=$ \texttt{valid} with the minimum update value for $d = 0$.
  \item For $i = 1, \hdots, n - 1$:
    \begin{enumerate}
    \item For each \texttt{valid} $p_i$ close enough to
      $p_0, \hdots, p_{i-1}$ (see
      \cref{ssec:bottom-up-search}):\label{item:bottom-up-for}
      \begin{enumerate}
      \item Do the update corresponding to $(p_0, \hdots, p_i)$ and
        keep track of the minimizing $\lambda^* \in \Delta^i$. This
        update can optionally be skipped by first computing $\mu^*$
        corresponding to the optimum of the incident lower-dimensional
        update $(p_0, \hdots, p_{i-1})$ and checking if
        $\mu^* \geq 0$.
      \end{enumerate}
    \item Let $p_i$ be the node which forms the update with the
      minimum value.
    \end{enumerate}
  \end{enumerate}
\end{algorithm}

In the next section, we conduct numerical tests to compare the
performance of these two algorithms.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sisc-eikonal.tex"
%%% End:
