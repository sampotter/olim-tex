\documentclass{article}

\usepackage{eikonal}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage[parfill]{parskip}

\begin{document}

\includegraphics[width=0.3\linewidth]{logo_umd.pdf}

\vspace{5.5em}

Dear Editor,

\vspace{1.5em}

Thank you for considering our manuscript:
\vspace{1em}
\begin{center}
  ``Ordered Line Integral Methods for Solving the Eikonal Equation.''
\end{center}
\vspace{1em} We are grateful to the reviewers for their valuable
feedback and and for pointing out key references that we have
overlooked.  We have addressed all points raised by the reviewers, and
believe that the revised manuscript is clearer and easier to follow,
and that the updated bibliography is more complete.

\vspace{1em}

\section*{Summary of Changes}

We have made a major revision of our manuscript that simplifies and
streamlines the presentation, and highlights our contributions more
clearly and prominently.

We list a summary of the main changes in the new version of our paper
here:

\begin{itemize}

\item References suggested by the reviewers were added in appropriate
  places with
  commentary~\cite{treister2016fast,zhang2006high,bornemann2006finite,luo2012fast}. These
  references were very interesting and relevant, and we appreciate the
  reviewers recommending them to us.

\item We added a new figure (fig. 1) that presents the family of
  algorithms studied in the paper as a decision tree, which we hope
  will focus readers' attention on the main contributions and focus of
  the paper. This addresses misunderstandings in the two reviews we
  received.

\item We improved the wording and organization of our ``Results''
  section to focus it and frontload our results, making them more
  obvious.

\item The position of Figures 1 and 2 were adjusted to better match
  the flow of the text.

\item We remade Figure 2 (now 3) to make it more comprehensible and
  useful.

\item We remade Figure 3 (now 4) to make it easier for the reader to
  understand how the update simplexes relate to the grid of nodes
  $\mathcal{G}$ by adding some grid lines.

\item We rewrote sections 3.1 and 3.2 and combined them into a new
  section (``Approximating the action functional'').

\item We changed our presentation of the cost functions and separated
  our presentation of the $\theta$-rules to make a clear distinction
  between the cost functions $\Frhr, \Fmpzero$, and $\Fmpone$, which
  are used in our solvers, and the $\theta$-rules, which generalize
  the cost functions, and are used to simplify our proofs.

\item We deleted some paragraphs that we felt were unnecessary and
  could be omitted since they did not contribute anything to the flow
  of ideas in the paper.

\item We have made a number of small fixes and notational changes to
  ``debug'' typos and otherwise improve the readability of the paper.

\item We added a new figure (Figure 5) that we believe addresses a
  central point about the \texttt{mp0} quadrature rule that is not
  made well enough in the text, and is easiest to convey graphically.

\item We made minor improvements to Figure 4 (now 6).

\item We simplified the presentation of neighborhood selection for the
  \emph{bottom-up} algorithm.

\item We moved the section on skipping updates using the KKT
  conditions to an appendix. We believe the idea is not too
  complicated, and that the reader interested in implementing the
  method should find the details in the appendix sufficient.

\end{itemize}

\vspace{1em}

\section*{Response to Reviewer \#1: \texttt{review\_JOMP\_Eikonal.pdf}}

We address the detailed comments first:

\begin{itemize}

\item \textbf{Reviewer:} Overall, there is no precise algorithm
  explaining the proposed method, and the description is a bit hard to
  follow. Section 3.2 is the key, and is not written clearly. Where do
  Eq. (7) and Eq. (8) come from? What is $\ell_\lambda$? What is
  $\hat{s}$? The explanation later regarding $\theta$ is a bit
  vague. Please revise and write the method more clearly, and consider
  complementing the description with an algorithm. It may be that
  Algorithms 2 and 3 are the algorithms, but they appear much
  later. Please try to clarify.

  \textbf{Response:} The precise algorithm is Algorithm 1 (the generic
  Dijkstra-like algorithm) whose Step 3d is done as described in
  either Algorithm 2 (\emph{top-down}) or Algorithm 3
  (\emph{bottom-up}). Individual updates are done according to the
  quadrature rule selected (either \texttt{mp0}, \texttt{mp1}, or
  \texttt{rhr})---that is, either using a QR decomposition in the of
  \texttt{rhr} and \texttt{mp0}, or an iterative solver in the case of
  \texttt{mp1}. We have added a figure (Figure 1) to emphasize this
  point.

  Regarding notational confusion, we rewrote and combined sections 3.1
  and 3.2 in a manner which we hope is easier to understand: see the
  new section 3.1. We also made some notational changes which we
  believe are improvements. In particular, regarding specific issues:
  \begin{itemize}
  \item Previously, we used the notation
    $l_\lambda = \|\hat{p} - p_\lambda\| = \|p_\lambda\|$. We have
    chosen to remove this notation and replace it with $\|p_\lambda\|$
    to reduce confusion.
  \item We use the notation $\hat{s}$ for the $s$ evaluated at the
    grid point in $\mathcal{G}$ associated with $\hat{p}$. We use the
    hat (\^{}) notation in several places ($\hat{p}$ and $\hat{U}$)
    and believe it is more helpful than confusing, so have decided to
    keep it. We have added a few more reminders explaining the meaning
    of the hat.
  \end{itemize}

\item \textbf{Reviewer:} I'm trying to figure out the differences
  between the proposed methods: rhr is equivalent to the standard FMM
  - that is using a 1st order upwind scheme for the integration. mp1
  applies the same, only with a midpoint rule. mp1 requires a few
  iterations. mp0 starts like mp1, but only one Newton's iteration is
  applied for minimization. Is that correct? In any case, please try
  to further clarify that in the text.

  \textbf{And:} Is mp1 similar to Fast Marching in a way?

  \textbf{And:} I did not understand the algorithms: top-down and
  bottom-up. Please try to make this clearer. In what sense is it
  top-down or bottom-up? The order in which the grid points are
  approached?

  \textbf{Response:} To answer the question, the \texttt{rhr} and
  \texttt{mp0} methods both use a QR decomposition to find the
  minimizing arguments of local functional minimization problems
  (Theorem 2). Neither use any iterative scheme. Then, \texttt{mp0}
  uses $\Fmpone$ for the ultimate function evaluation to compute
  $\hat{U}$, which makes it fast, accurate, and consistent
  (\texttt{rhr} does \emph{not} do this). This is justified by Theorem
  1. By contrast, the \texttt{mp1} method uses an iterative solver for
  each update. This idea is what allows \texttt{mp0} to combine the
  speed of \texttt{rhr} with the accuracy of \texttt{mp1}. The choice
  of quadrature rule is one of three splits in our OLIM family of
  eikonal solvers. The other two are the choice of \emph{top-down} or
  \emph{bottom-up} update algorithm, and the neighborhood size (in the
  case of \emph{top-down}).

  To address this criticism, we have tried to make this ideas more
  pronounced. We have included Figure 1 and have placed greater
  emphasis more the roles of the quadrature rules and
  \emph{top-down}/\emph{bottom-up} algorithms.

\item \textbf{Reviewer:} Numerical Results: what does olim stand for?

  \textbf{Response:} OLIM stands for Ordered Line Integral Methods.

\item \textbf{Reviewer:} Numerical Results: It seems that the
  integration approach itself (in rhr) is slower than FMM) but the
  variant mp0, which is comparable in speed to rhr, is more accurate
  than FMM, even though we do not solve the minimization exactly. Is
  that right? So the real method this paper is trying to sell is
  mp0. It will be better if the authors make this clearer.

  \textbf{Response:} Yes, \texttt{mp0} is the best quadrature rule. To
  clarify the difference between the FMM and our solvers, we have
  added some additional explanation. The comparison is made explicit
  in Theorems 2 and 3.

  In addition, we have attempted to restructure our paper to feature
  our main results more prominently.

\item \textbf{Reviewer:} Missing literature: there are a few works on
  high order approaches in Fast Sweeping based on third order ENO and
  WENO schemes - these can reduce the computational grid
  dramatically. Also missing are works on parallel Fast Sweeping
  Approaches. In addition, the paper ``A fast marching algorithm for
  the factored eikonal equation'' seems very relevant here.

  \textbf{Response:} We apprecriate the reviewer suggesting these
  references; we were unaware of the referenced paper, and have
  included it~\cite{treister2016fast}. Additionally, we have included
  one of the main papers on higher-order WENO/ENO schemes combined
  with the fast sweeping method~\cite{zhang2006high}.

  We would also like to point out that \cite{treister2016fast} uses a
  fast marching method based on a finite difference discretization of
  the \emph{multiplicatively} factored eikonal equation; by contrast,
  we use a semi-Lagrangian Dijkstra-like algorithm more similar to
  Tsitsiklis's algorithm~\cite{tsitsiklis1995efficient} and apply it
  to the \emph{additively} factored eikonal equation.

\end{itemize}

Additionally, some minor comments were presented:

\begin{itemize}

\item \textbf{Reviewer:} Section 3: First sentence: ``The fast
  marching method [42] discretizes the eikonal equation,''. This is
  not an accurate sentence. The method does not discretize the
  equation... Please revised.

  \textbf{Response:} Fixed.

\item \textbf{Reviewer:} Some figures (e.g., figure 1) are presented
  way too early in the paper and are not informative there. Figure 2
  is presented in page 3 but first referenced in page 6.

  \textbf{Response:} These figures have been moved to more appropriate
  places.

\item \textbf{Reviewer:} Page 8: ``In this work, we only use uniform
  square or cubic grids;'' Figure 3 is a bit misleading, showing
  tetrahedra. How this fit together with the sentence? Please try to
  clarify.

  \textbf{Response:} We have modified Figure 3 (now 4) to make it
  explicit how the simplexes arise and relate to the underlying cubic
  grid.

  The domain $\Omega$ is discretized into a uniform grid of points
  $\mathcal{G}$. In finite difference-based methods such as the FMM or
  WENO/ENO-style FSMs, there is a stencil of points surrounding each
  updated point.

  In semi-Lagrangian methods it is necessary to think of update
  simplexes that contain locally parametrized characteristics instead
  of stencils. This idea is in Tsitsiklis's original
  paper~\cite{tsitsiklis1995efficient}. We have tried to emphasize
  this contrast throughout the paper.

  We have modified our figures to show the grid of points
  $\mathcal{G}$ surrounding update simplexes (update triangles and
  tetrahedra). See Figures 3, 4, 5, 8, and 10. Figures 3 and 4 have
  been updated, and Figure 5 is new.

\end{itemize}

\vspace{1em}

\section*{Response to Reviewer \#2: \texttt{review.pdf}}

\begin{itemize}

\item \textbf{Reviewer:} The local variational minimization approach
  has been proposed and used before, see [\emph{Bornemann \&
    Rasch}]. So the idea is not new.

  \textbf{Response:} In the paper, we do not claim that using the
  local variational minimization approach is new. In fact, it has been
  present since Tsitsiklis's original
  work~\cite{tsitsiklis1995efficient} on Dijkstra-like algorithms, so
  it in fact predates Sethian's work on the fast marching
  method~\cite{sethian1996fast}. That is, the idea predates the
  referenced paper~\cite{bornemann2006finite} by at least 21 years. We
  explain this point in the introduction (see the first paragraph of
  Section 1.1).

  However, the paper by Bornemann \& Rasch is of particular interest
  to us, and we thank the reviewer for bringing it to our
  attention~\cite{bornemann2006finite}. We have added a citation in an
  appropriate place.

  We note that Bornemann \& Rasch presents complementary results which
  \emph{do not} significantly overlap with our own. Our emphasis is on
  fast solvers and the attendant numerical developments. E.g., the
  referenced paper only shows how to do the Hopf-Lax update in 2D for
  an equivalent of our \texttt{rhr} quadrature rule, and uses an
  abundance of trigonometry to do so. We show in detail how to do the
  Hopf-Lax update in any dimension using linear algebra or numerical
  optimization, and for three quadrature rules (\texttt{rhr},
  \texttt{mp0}, \texttt{mp1}). We also prove results showing the
  interrelation of these quadrature results, and provide fast
  algorithms (``top-down''/``bottom-up'') that allow one to
  efficiently perform \emph{all} updates for a newly \texttt{valid}
  point.

  As an aside, we also point out that the numerical results presented
  in Bornemann \& Rasch are unimpressive. Figures 3 and 4 in their
  paper clearly indicate that for moderate to large problem sizes,
  Vladimirsky's ordered upwind method is faster and more
  accurate~\cite{sethian2003ordered}.

\item \textbf{Reviewer:} The equivalence of the local variational
  minimization approach and the upwind finite difference scheme, as
  well as the convexity of the local functional under piecewise linear
  approximation has been proved previously, see [...]. So part of the
  theoretical support is not new.

  \textbf{Response:} The equivalence of the variational approach and
  the finite difference scheme is an old result,
  see~\cite{sethian1999level}. We do comment on this in the revision:
  see the paragraph preceding Theorem 3. The convexity of the local
  functional as it is approximated is a minor result in our overall
  work and we do not claim that it is our central result. Since we use
  it, we include it for completeness and context.

\item \textbf{Reviewer:} The authors must show new/novel contributions
  other than existing formulations/ideas in the framework of the fast
  marching method.

  \textbf{Response:} We have made a major revision of our presentation
  that, we hope, highlights our contributions clearly.

\end{itemize}

\vspace{2em}

Enclosed is the revised manuscript. We look forward to hearing from you.

\vspace{1em}

Sincerely,

\begin{minipage}{.5\linewidth}
  \includegraphics[width=.6\linewidth]{sig.pdf}
\end{minipage}%
\begin{minipage}{.5\linewidth}
\end{minipage}


\begin{minipage}[t]{.5\linewidth}
  Samuel Potter \\
  Graduate Student \\
  Department of Computer Science \\
  University of Maryland \\
  Brendan Iribe Center \\
  College Park, MD, 20742 \\
  \texttt{sfp@umiacs.umd.edu}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
  Maria Cameron \\
  Associate Professor \\
  Department of Mathematics \\
  University of Maryland \\
  Kirwan Hall \\
  College Park, MD, 20742 \\
  \texttt{cameron@math.umd.edu}
\end{minipage}

\bibliographystyle{plain}
\bibliography{eikonal}{}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
