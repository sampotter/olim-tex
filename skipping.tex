\documentclass[eikonal.tex]{subfiles}

\begin{document}

\section{Skipping Triangles}

We are interested in when:
\begin{align}\label{eq:constrained-and-unconstrained-tri-agrees}
  \arg\min_{0 \leq \lambda \leq 1} F_0(\lambda; \theta) = \lambdaopt = \arg\min_{\lambda \in \R} F_0(\lambda; \theta).
\end{align}
When the two disagree, we can safely skip over triangle updates.

\begin{lemma}\label{lemma:positive-second-derivative-of-F}
  If $\delta p$ and $p_0$ are linearly independent, then
  $F''_0(\lambda; \theta) > 0$ for all $\lambda, \theta \in [0, 1]$.
\end{lemma}

\begin{proof}
  We have:
  \begin{align*}
    F_0''(\lambda; \theta) = \frac{s_{\theta}h}{2 l(\lambda)} \parens{q''(\lambda) - \frac{{q'(\lambda)}^2}{2q(\lambda)}}.
  \end{align*}
  In our case, $l(\lambda)$ is the length of a nonzero vector, giving
  $l(\lambda) > 0$; so, we examine
  $2q(\lambda)q''(\lambda) - {(q'(\lambda))}^2 > 0$. This is the same
  as $4a(a\lambda^2 + 2b\lambda + c) - 4(a\lambda + b)^2 > 0$, which
  is satisfied if $ac > b^2$. This is equivalent to the Gram matrix
  for $\set{\delta p, p_0}$ being nonsingular. Since a Gram matrix is
  nonsingular if and only if the associated Gram vectors are linearly
  independent, the result follows.
\end{proof}

For all of our triangular updates,
\cref{lemma:positive-second-derivative-of-F} applies, so that
$F''_0 > 0$. Then, if $F'_0(0; \theta)$ and $F'_0(1; \theta)$ have the
same sign, we can conclude that the minimizer of $F_0$ lies outside
the interval $[0, 1]$. Since $F'_0$ is strictly increasing, we just
check if $F'_0(0; \theta) > 0$ or $F'_0(1; \theta) < 0$.

\begin{lemma}
  Let $\alpha = -\delta u/(s_\theta h)$. Then,
  $(\delta p, p_0)/\norm{p_0}_2 \leq \alpha \leq (\delta p,
  p_1)/\norm{p_1}_2$ implies
  \cref{eq:constrained-and-unconstrained-tri-agrees}.
\end{lemma}

\begin{proof}
  Since $q'(\lambda) = 2(\delta p, p_\lambda)$, we have:
  \begin{align*}
    F_0'(\lambda; \theta) = \delta u + s_\theta h \frac{(\delta p, p_\lambda)}{\norm{p_\lambda}}
  \end{align*}
  Stiuplating that $F'_0(0; \theta) \leq 0$ and
  $F'(0; \theta) \geq 0$, we get the result.
\end{proof}

\section{Skipping Simplexes}

In this section, we will determine conditions under which general
simplex updates can be skipped. We will work with the simplex which
generalizes the unit interval:
\begin{align*}
  \Delta^d = \set{\lambda = (\lambda_1, \hdots, \lambda_d) \in \R^d : \lambda \geq 0 \mbox{ and } 1^\top \lambda \leq 1}.
\end{align*}
This simplex defines the parameter space for our minimization problem,
which can be related to the points $p_0, \hdots, p_d$,
which---together with the origin---form the vertices of the simplex
containing the path of the line integral to be minimized.

Let $\lambda_0 = 1 - 1^\top \lambda$. The point $p_\lambda$ denotes
the convex combination of the points $p_0, \hdots, p_d$ with convex
coefficients $\lambda_0, \hdots, \lambda_d$. Defining
$\delta p_i = p_i - p_0$ and the matrix
$\delta P \in \mathbb{R}^{d + 1 \times d}$ by:
\begin{align*}
  \delta P = \begin{bmatrix} \delta p_1 & \cdots & \delta p_d \end{bmatrix},
\end{align*}
we can see that $p_\lambda = \delta P \lambda + p_0$, which gives us a
useful means of converting between the two spaces (this is just a
parametrization of the dimension $d - 1$ affine hull of the points
$p_0, \hdots, p_d$ in terms of the convex coefficients
$\lambda_1, \hdots, \lambda_d$).

The function to be minimized is
given by:
\begin{align*}
  F(\lambda) = u(\lambda) + s_\theta h l(\lambda),
\end{align*}
where $u(\lambda) = \lambda_0 u_0 + \cdots + \lambda_d \lambda_d$ and
$l(\lambda) = \sqrt{Q(\lambda)} = \sqrt{p_\lambda^\top
  p_\lambda}$. The following theorem generalizes
\cref{lemma:positive-second-derivative-of-F}.

\begin{theorem}
  If $\set{p_0, \delta p_1, \hdots, \delta p_d}$ are linearly
  independent, then $\nabla^2 F > 0$ for all $\lambda$.
\end{theorem}

\begin{proof}
  We have:
  \begin{align*}
    \nabla^2 F = \frac{s_\theta h}{2l} \parens{\nabla^2 Q - \frac{1}{2 Q} \nabla Q \nabla Q^\top}.
  \end{align*}
  Since $l > 0$, we have $\nabla^2 F > 0$ if
  $2 Q \nabla^2 Q > \nabla Q \nabla Q^\top$ holds. Since we can write
  $\nabla^2 Q = 2 \delta P^\top \delta P$ and
  $\nabla Q = 2 \delta P^\top p_\lambda$, it follows that
  $p_\lambda^\top p_\lambda \delta P^\top \delta P > \delta P^\top
  p_\lambda p_\lambda^\top \delta P$ implies $\nabla^2 F > 0$. Now,
  consider the following Gram matrix:
  \begin{align*}
    \begin{bmatrix}
      \delta P^\top \delta P & \delta P^\top p_\lambda \\
      p_\lambda^\top \delta P & p_\lambda^\top p_\lambda
    \end{bmatrix} = \begin{bmatrix}
      \delta P & p_\lambda
    \end{bmatrix}^\top \begin{bmatrix}
      \delta P & p_\lambda
    \end{bmatrix}.
  \end{align*}
  Gram matrices are positive definite if and only if the corresponding
  Gram vectors form a basis. Since
  $p_\lambda \in p_0 + \langle \delta p_1, \hdots, \delta p_d \rangle$
  and since $\set{p_0, \delta p_1, \hdots, \delta p_d}$ are linearly
  independent by assumption, we can see that this Gram matrix is
  indeed positive definite. Hence, the Schur complement of this matrix
  with respect to $p_\lambda^\top p_\lambda$ satisfies:
  \begin{align*}
    \delta P^\top \delta P - \frac{\delta P^\top p_\lambda p_\lambda^\top \delta P}{p_\lambda^\top p_\lambda} > 0,
  \end{align*}
  giving us the result.
\end{proof}

\begin{remark}
  When $\nabla^2 F > 0$ holds everywhere, it follows that $F$ is a
  strictly convex function and has a global minimizer. Our skipping
  procedure amounts to determining whether:
  \begin{align}\label{eq:minimizers-are-the-same}
    \operatorname{arg}\;\min_{\Delta^d} F(\lambda) = \operatorname{arg}\;\min_{\R^d} F(\lambda).
  \end{align}
\end{remark}

For triangular updates, we saw that we could check the values of $F'$
at the endpoints of $[0, 1]$ to determine whether the interval
$[0, 1]$ contained the global minimizer of $F$. To generalize this to
$\Delta^d$, we need a short lemma.

\begin{lemma}
  Let $X \subseteq \R^d$ be a compact convex set, let
  $F : \R^d \to \R$ be a smooth function such that $\nabla^2 F > 0$,
  let $\lambdaopt$ be the global minimizer of $F$, and let
  $\lambda_0 \in \partial X$. If
  $\nabla F(\lambda_0)^\top \lambda \geq 0$ for every $\lambda \in X$,
  then either $\lambdaopt = \lambda_0$ or $\lambdaopt \notin X$.
\end{lemma}

\begin{proof}
  Write $g_0 = \nabla F(\lambda_0)$, and define
  $H_{\geq} = \lambda_0 \,+\, \{\lambda: g_0^\top \lambda \geq 0\}$
  and
  $H_{\leq} = \lambda_0 \,+\, \{\lambda: g_0^\top \lambda \leq 0\}$.
  By assumption, $X \subseteq H_{\geq}$. Let
  $L = \set{\lambda : F(\lambda) \leq F(\lambda_0)}$ be the
  $F(\lambda_0)$-sublevel set of $F$. Since $\nabla^2 F > 0$ and
  $g_0^\top \lambda_0 \geq 0$, we have $L \subseteq H_{\leq}$. Lastly,
  because $X \cap L \subseteq H_{\leq}$, we have that
  $\lambdaopt \in X$ implies $\lambdaopt = \lambda_0$. The result
  follows.
\end{proof}

\begin{theorem}
  Let $\alpha_i = -\delta u_i/(s_{\theta}h)$ and
  $\alpha = (\alpha_1, \hdots, \alpha_m)$. Then:
  \begin{align*}
    \arg\min_{\lambda \in \Delta^m} F_0(\lambda; \theta) = \arg\min_{\lambda \in \R^m} F_0(\lambda; \theta)
  \end{align*}
  if and only if:
  \begin{align*}
    \delta P^\top \frac{p_0}{\norm{p_0}} \leq \alpha \leq \delta P^\top \frac{\overline{p}}{\norm{\overline{p}}}
  \end{align*}
  where $\overline{p} = (p_1 + \cdots + p_m)/m$.
\end{theorem}

\begin{proof}
  Let $e_i \in \R^m$ denote the standard basis vectors for
  $i = 1, \hdots, m$ and let $e_0 = 0$. We observe that
  $p_{e_i} = p_i$ and $l(e_i) = \norm{p_i}$ for $i = 0, \hdots,
  m$. Hence:
  \begin{align*}
    \nabla F(e_i) = \delta u + \frac{s_\theta h}{\norm{p_i}} \delta P^\top p_i.
  \end{align*}
  For $i = 0$, we require $\nabla F(e_0) \leq 0$ componentwise, giving:
  \begin{align*}
    \delta u - s_\theta h \delta P^\top p_0/\norm{p_0} \leq 0  \iff -\alpha \leq \delta P^\top p_0/\norm{p_0}
  \end{align*}
  For $i, j = 1, \hdots, d$, we find
  $\nabla F(e_i)^\top e_j = \delta u_j + s_\theta h \delta p_i^\top
  p_j$ for $i, j = 1, \hdots, d$. Further, if
  $\nabla F(e_i)^\top e_j \leq 0$, this gives
  $\delta P^\top p_i \leq \alpha$ for $i = 1, \hdots, d$.
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
