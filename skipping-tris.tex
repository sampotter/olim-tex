\documentclass{article}

\usepackage{eikonal}

\begin{document}

\large

\section{Types of Triangular Updates}

For the two- and three-dimensional ordered line integral methods
(OLIMs), we are concerned with five different triangular updates. If
we let $p_0, p_1$ be the points forming the vertices of the triangle
that contains the path of our line integral, then we can identify each
type of update with the coefficients of a quadratic form.

First, let $\delta p = p_1 - p_0$. Then, define
$q(\lambda) = \norm{p_0 + \lambda \delta p}_2^2 = a\lambda^2 +
2b\lambda + c$. Since:
\begin{align*}
  p_0 + \lambda \delta p = \begin{bmatrix} \delta p & p_0 \end{bmatrix} \begin{bmatrix} \lambda \\ 1 \end{bmatrix},
\end{align*}
we can write:
\begin{align*}
  q(\lambda) = \begin{bmatrix} \lambda \\ 1 \end{bmatrix}^\top \begin{bmatrix} \delta p & p_0 \end{bmatrix}^\top \begin{bmatrix} \delta p & p_0 \end{bmatrix} \begin{bmatrix} \lambda \\ 1 \end{bmatrix} = \begin{bmatrix} \lambda \\ 1 \end{bmatrix} \begin{bmatrix} (\delta p, \delta p) & (\delta p, p_0) \\ (p_0, \delta p) & (p_0, p_0) \end{bmatrix} \begin{bmatrix} \lambda \\ 1 \end{bmatrix},
\end{align*}
i.e. $a = (\delta p, \delta p)$, $b = (\delta p, p)$, and
$c = (p, p)$. So, we can see that $q(\lambda)$ is a quadratic form
determined by the Gram matrix for the vectors $\set{\delta p,
  p_0}$. This generalizes immediately to tetrahedral and
higher-dimensional simplex updates.

We can identify each triangular update by the Hamming weights of $p_0$
and $p_1$. For example, in two dimensions, the triangular updates for
OLIM4 use $p_0 = (1, 0)$ and $p_1 = (0, 1)$, both of which have
Hamming weight 1; so, we refer to this triangular update as
\texttt{tri11}. Altogether, our two- and three-dimensional OLIMs use
five triangular updates, whose coefficients are listed below:
\begin{center}
  \begin{tabular}{c|ccccc}
    Method & \texttt{tri11} & \texttt{tri12} & \texttt{tri13} & \texttt{tri22} & \texttt{tri23} \\
    $a$ & 2 & 1 & 2 & 2 & 1 \\
    $b$ & -1 & 0 & 0 & -1 & 0 \\
    $c$ & 1 & 1 & 1 & 2 & 2
  \end{tabular}
\end{center}
The different OLIM methods make use of these triangular updates as
follows:
\begin{center}
  \begin{tabular}[h]{cc}
    OLIM & Types \\
    \midrule
    OLIM4, OLIM6 & \texttt{tri11} \\
    OLIM8 & \texttt{tri12} \\
    OLIM18 & \texttt{tri12}, \texttt{tri22} \\
    OLIM26 & \texttt{tri13}, \texttt{tri23}
  \end{tabular}
\end{center}

\section{Skipping Triangles}

In this section, let $u(\lambda) = (1 - \lambda) u_0 + \lambda u_1$
and let $l(\lambda) = \sqrt{q(\lambda)}$.

\begin{lemma}\label{lemma:positive-second-derivative-of-F}
  Let $F(\lambda) = u(\lambda) + \hat{s} h l(\lambda)$, where
  $\hat{s}$ and $h$ are positive constants. Then, $F'' > 0$ if
  $\delta p$ and $p_0$ are linearly independent.
\end{lemma}

\begin{proof}
  We have:
  \begin{align*}
    F''(\lambda) = \frac{\hat{s}h}{2 l(\lambda)} \parens{q''(\lambda) - \frac{{q'(\lambda)}^2}{2q(\lambda)}}.
  \end{align*}
  In our case, $l(\lambda)$ is the length of a nonzero vector, giving
  $l(\lambda) > 0$; so, we examine
  $2q(\lambda)q''(\lambda) - {(q'(\lambda))}^2 > 0$. This is the same
  as $4a(a\lambda^2 + 2b\lambda + c) - 4(a\lambda + b)^2 > 0$, which
  is satisfied if $ac > b^2$. This is equivalent to the Gram matrix
  for $\set{\delta p, p_0}$ being nonsingular. Since a Gram matrix is
  nonsingular if and only if the associated Gram vectors are linearly
  independent, the result follows.
\end{proof}

We would like to know when the minimizer of $F(\lambda)$ lies in
$(0, 1)$. For all of our triangular updates,
\cref{lemma:positive-second-derivative-of-F} applies, so that
$F'' > 0$ everywhere. Then, if $F'(0)$ and $F'(1)$ have the same sign,
we can conclude that the minimizer of $F$ lies outside the interval
$(0, 1)$. Since $F'$ is strictly increasing, we can more simply check
if $F'(0) > 0$ or $F'(1) < 0$.

\begin{lemma}
  Let $\rho_i = (\delta p, p_i)/\norm{p_i}_2$ for $i = 0, 1$ and let
  $\alpha = -\delta u/(\hat{s}h)$. Then, a necessary condition for
  $\lambdaopt \in (0, 1)$ is $\alpha \in (\rho_0, \rho_1)$.
\end{lemma}

\begin{proof}
  We have
  $F'(\lambda) = \delta u + \hat{s} h q'(\lambda)/(2 l(\lambda))$. Then:
  \begin{align*}
    F'(0) = \delta u + \hat{s} h b / \sqrt{c}, \qquad F'(1) = \delta u + \hat{s} h \frac{a + b}{\sqrt{a + 2b + c}}.
  \end{align*}
  Furthermore,
  $a + b = (\delta p, \delta p) + (\delta p, p_0) = (\delta p, p_1)$,
  and similarly $b + c = (p_0, p_1)$. Hence,
  $a + 2b + c = (p_1, p_1)$. Altogether, we have:
  \begin{align*}
    \frac{b}{\sqrt{c}} = \frac{(\delta p, p_0)}{\norm{p_0}_2}, \qquad \frac{a + b}{a + 2b + c} = \frac{(\delta p, p_1)}{\norm{p_1}_2}.
  \end{align*}
  The result follows from rewriting the condition ``$F'(0) > 0$ or
  $F'(1) < 0$'', applying these identities to $F'(0)$ and $F'(1)$.
\end{proof}

Note that $\rho_i$ above is just the scalar projection of $p_i$ in the
direction of $\delta p$.

Using our values for the coefficients $a, b$, and $c$ for our
triangular updates, we get the following table of values for $\rho_0$
and $\rho_1$:
\begin{center}
  \begin{tabular}{c|ccccc}
    Method & \texttt{tri11} & \texttt{tri12} & \texttt{tri13} & \texttt{tri22} & \texttt{tri23} \\
    $\rho_0$ & -1 & 0 & 0 & -1/2 & 0 \\
    $\rho_1$ & 1 & 1/2 & 2/3 & 1/2 & 1/3
  \end{tabular}
\end{center}

\section{Skipping Simplexes}

In this section, we will determine conditions under which general
simplex updates can be skipped. We will work with the simplex which
generalizes the unit interval:
\begin{align*}
  \Delta^d = \set{\lambda = (\lambda_1, \hdots, \lambda_d) \in \R^d : \lambda \geq 0 \mbox{ and } 1^\top \lambda \leq 1}.
\end{align*}
This simplex defines the parameter space for our minimization problem,
which can be related to the points $p_0, \hdots, p_d$,
which---together with the origin---form the vertices of the simplex
containing the path of the line integral to be minimized.

Let $\lambda_0 = 1 - 1^\top \lambda$. The point $p_\lambda$ denotes
the convex combination of the points $p_0, \hdots, p_d$ with convex
coefficients $\lambda_0, \hdots, \lambda_d$. Defining
$\delta p_i = p_i - p_0$ and the matrix
$\delta P \in \mathbb{R}^{d + 1 \times d}$ by:
\begin{align*}
  \delta P = \begin{bmatrix} \delta p_1 & \cdots & \delta p_d \end{bmatrix},
\end{align*}
we can see that $p_\lambda = \delta P \lambda + p_0$, which gives us a
useful means of converting between the two spaces (this is just a
parametrization of the dimension $d - 1$ affine hull of the points
$p_0, \hdots, p_d$ in terms of the convex coefficients
$\lambda_1, \hdots, \lambda_d$).

The function to be minimized is
given by:
\begin{align*}
  F(\lambda) = u(\lambda) + \hat{s} h l(\lambda),
\end{align*}
where $u(\lambda) = \lambda_0 u_0 + \cdots + \lambda_d \lambda_d$ and
$l(\lambda) = \sqrt{Q(\lambda)} = \sqrt{p_\lambda^\top
  p_\lambda}$. The following theorem generalizes
\cref{lemma:positive-second-derivative-of-F}.

\begin{theorem}
  If $\set{p_0, \delta p_1, \hdots, \delta p_d}$ are linearly
  independent, then $\nabla^2 F > 0$ for all $\lambda$.
\end{theorem}

\begin{proof}
  We have:
  \begin{align*}
    \nabla^2 F = \frac{\hat{s}h}{2l} \parens{\nabla^2 Q - \frac{1}{2 Q} \nabla Q \nabla Q^\top}.
  \end{align*}
  Since $l > 0$, we have $\nabla^2 F > 0$ if
  $2 Q \nabla^2 Q > \nabla Q \nabla Q^\top$ holds. Since we can write
  $\nabla^2 Q = 2 \delta P^\top \delta P$ and
  $\nabla Q = 2 \delta P^\top p_\lambda$, it follows that
  $p_\lambda^\top p_\lambda \delta P^\top \delta P > \delta P^\top
  p_\lambda p_\lambda^\top \delta P$ implies $\nabla^2 F > 0$. Now,
  consider the following Gram matrix:
  \begin{align*}
    \begin{bmatrix}
      \delta P^\top \delta P & \delta P^\top p_\lambda \\
      p_\lambda^\top \delta P & p_\lambda^\top p_\lambda
    \end{bmatrix} = \begin{bmatrix}
      \delta P & p_\lambda
    \end{bmatrix}^\top \begin{bmatrix}
      \delta P & p_\lambda
    \end{bmatrix}.
  \end{align*}
  Gram matrices are positive definite if and only if the corresponding
  Gram vectors form a basis. Since
  $p_\lambda \in p_0 + \langle \delta p_1, \hdots, \delta p_d \rangle$
  and since $\set{p_0, \delta p_1, \hdots, \delta p_d}$ are linearly
  independent by assumption, we can see that this Gram matrix is
  indeed positive definite. Hence, the Schur complement of this matrix
  with respect to $p_\lambda^\top p_\lambda$ satisfies:
  \begin{align*}
    \delta P^\top \delta P - \frac{\delta P^\top p_\lambda p_\lambda^\top \delta P}{p_\lambda^\top p_\lambda} > 0,
  \end{align*}
  giving us the result.
\end{proof}

\begin{remark}
  When $\nabla^2 F > 0$ holds everywhere, it follows that $F$ is a
  strictly convex function and has a global minimizer. Our skipping
  procedure amounts to determining whether:
  \begin{align}\label{eq:minimizers-are-the-same}
    \operatorname{arg}\;\min_{\Delta^d} F(\lambda) = \operatorname{arg}\;\min_{\R^d} F(\lambda).
  \end{align}
\end{remark}

For triangular updates, we saw that we could check the values of $F'$
at the endpoints of $[0, 1]$ to determine whether the interval
$[0, 1]$ contained the global minimizer of $F$. To generalize this to
$\Delta^d$, we need a short lemma.

\begin{lemma}
  Let $X \subseteq \R^d$ be a compact convex set, let
  $F : \R^d \to \R$ be a smooth function such that $\nabla^2 F > 0$,
  let $\lambdaopt$ be the global minimizer of $F$, and let
  $\lambda_0 \in \partial X$. If
  $\nabla F(\lambda_0)^\top \lambda \geq 0$ for every $\lambda \in X$,
  then either $\lambdaopt = \lambda_0$ or $\lambdaopt \notin X$.
\end{lemma}

\begin{proof}
  Write $g_0 = \nabla F(\lambda_0)$, and define
  $H_{\geq} = \lambda_0 \,+\, \{\lambda: g_0^\top \lambda \geq 0\}$
  and
  $H_{\leq} = \lambda_0 \,+\, \{\lambda: g_0^\top \lambda \leq 0\}$.
  By assumption, $X \subseteq H_{\geq}$. Let
  $L = \set{\lambda : F(\lambda) \leq F(\lambda_0)}$ be the
  $F(\lambda_0)$-sublevel set of $F$. Since $\nabla^2 F > 0$ and
  $g_0^\top \lambda_0 \geq 0$, we have $L \subseteq H_{\leq}$. Lastly,
  because $X \cap L \subseteq H_{\leq}$, we have that
  $\lambdaopt \in X$ implies $\lambdaopt = \lambda_0$. The result
  follows.
\end{proof}

\begin{theorem}
  Let $\alpha_i = -\delta u_i/(\hat{s}h)$ and
  $\alpha = (\alpha_1, \hdots, \alpha_d)$. Then, a necessary condition
  for \cref{eq:minimizers-are-the-same} to hold is
  $\delta P^\top p_0 \leq \alpha$ and $\alpha \leq \delta P^\top p_i$
  for $i = 1, \hdots, d$.
\end{theorem}

\begin{proof}
  Let $e_i \in \R^d$ denote the standard basis vectors for
  $i = 1, \hdots, d$ and let $e_0 = 0$. We observe that
  $p_{e_i} = p_i$ and $l(e_i) = \norm{p_i}$ for $i = 0, \hdots,
  d$. Hence:
  \begin{align*}
    \nabla F(e_i) = \delta u + \frac{\hat{s}h}{\norm{p_i}} \delta P^\top p_i.
  \end{align*}
  For $i = 0$, we require $\nabla F(e_0) \leq 0$ componentwise, giving:
  \begin{align*}
    \delta u - \hat{s}h \delta P^\top p_0/\norm{p_0} \leq 0  \iff -\alpha \leq \delta P^\top p_0/\norm{p_0}
  \end{align*}
  For $i, j = 1, \hdots, d$, we find
  $\nabla F(e_i)^\top e_j = \delta u_j + \hat{s} h \delta p_i^\top
  p_j$ for $i, j = 1, \hdots, d$. Further, if
  $\nabla F(e_i)^\top e_j \leq 0$, this gives
  $\delta P^\top p_i \leq \alpha$ for $i = 1, \hdots, d$.
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
