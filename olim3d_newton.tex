\documentclass{article}

\usepackage{eikonal}

\begin{document}

\section{Newton's Method with Exact Line Search for Tetrahedral Updates}

\subsection{Righthand Rule and ``Modified Midpoint'' Rule}

The righthand rule and modified midpoint tetrahedral updates can be
written in terms of a conic function. Letting
$\Delta^2 = \set{(\lambda_1, \lambda_2) \in \R : \lambda_1 \geq 0,
  \lambda_2 \geq 0, \lambda_1 + \lambda_2 \leq 1}$, the minimization
problem associated with the tetrahedral update is:
\begin{align*}
  \hat{u} = \min_{\lambda \in \Delta^2} F_0(\lambda) := \min_{\lambda \in \Delta^2} \curlyb{u_\lambda + \int_{[\hat{p}, p_\lambda]} s d\sigma} = \mbox{error} + \min_{\lambda \in \Delta_2} \curlyb{u_\lambda + \hat{s} \norm{\hat{p} - p_\lambda}_2}.
\end{align*}
Then, if we factor the grid resolution out of the length
$\norm{\hat{p} - p_\lambda}$, we have:
\begin{align*}
  \norm{\hat{p} - p_\lambda} = h \sqrt{Q(\lambda)},
\end{align*}
where
$Q(\lambda) = A\lambda_1^2 + B\lambda_1\lambda_2 + C\lambda_2^2 +
D\lambda_1 + E\lambda_2 + F$ is a conic function.

Since working directly with this conic function will facilitate our
calculations, we present a few basic properties of conic
functions. First, define:
\begin{align*}
  M = \begin{bmatrix} A & B/2 \\ B/2 & C \end{bmatrix}, \qquad \epsilon = \begin{bmatrix} D/2 \\ E/2 \end{bmatrix}.
\end{align*}
Then, evaluating the conic $Q$ can be written in terms of a quadratic
form in homogeneous coordinates:
\begin{align*}
  Q(\lambda) = \begin{bmatrix} \lambda \\ 1 \end{bmatrix}^\top \begin{bmatrix}
    M & \epsilon \\ \epsilon^\top & F \end{bmatrix} \begin{bmatrix} \lambda \\ 1 \end{bmatrix}.
\end{align*}
Using this form for $Q$ and expanding
$Q(\lambda) = \langle \lambda, \lambda \rangle_M + 2 \langle \epsilon,
\lambda \rangle + F$, the gradient and Hessian of $Q$ with respect to
$\lambda$ are:
\begin{align*}
  \nabla_\lambda Q = \nabla_\lambda Q(\lambda) = 2M\lambda + 2\epsilon, \qquad \nabla_\lambda^2 Q = 2M.
\end{align*}

Our goal is to derive the relaxed Newton's iteration for minimizing
$F_0(\lambda)$, where the step size is taken to be the exact minimizer
in the line search. That is, letting $p$ denote the direction for a
given step, and defining $G(\alpha) = \lambda + \alpha p$, we would
like to compute:
\begin{align*}
  \alpha^* = \argmin_{\alpha \leq 0} F_0(G(\alpha)).
\end{align*}
Since we are working with Newton's method, we take
$p = {(\nabla_\lambda^2F_0(\lambda))}^{-1} \nabla_\lambda F_0(\lambda)$,
where $\lambda$ is the current iterate.

\begin{lemma}
  The gradient and Hessian of $F_0$ with respect to $\lambda$ are given
  by:
  \begin{align*}
    \nabla_\lambda F_0(\lambda) &= \delta u + \frac{\hat{s} h}{\sqrt{Q(\lambda)}} {(M\lambda + \epsilon)}, \\
    \nabla_\lambda^2 F_0(\lambda) &= \frac{\hat{s}h}{Q(\lambda)^{3/2}} \parens{Q(\lambda) M - {(M\lambda + \epsilon)}{(M\lambda + \epsilon)}^\top}.
  \end{align*}
\end{lemma}

\begin{proof}
  First, compute:
  \begin{align*}
    \nabla_\lambda \sqrt{Q(\lambda)} = \frac{1}{2 \sqrt{Q(\lambda)}} \nabla_\lambda Q(\lambda) = \frac{1}{\sqrt{Q(\lambda)}}\parens{M\lambda + \epsilon}.
  \end{align*}
  From this, it is easy to see that $\nabla_\lambda F_0(\lambda)$ has
  the desired form. For the Hessian, we write:
  \begin{align*}
    \nabla_\lambda^2 F_0(\lambda)
    &= \hat{s}h \nabla_\lambda \frac{1}{\sqrt{Q(\lambda)}}{(M\lambda + \epsilon)} = \hat{s}h \parens{\parens{\nabla_\lambda \frac{1}{\sqrt{Q(\lambda)}}} {(M\lambda + \epsilon)}^\top + \frac{1}{\sqrt{Q(\lambda)}} {(M\lambda + \epsilon)}},
  \end{align*}
  from which the result readily follows.
\end{proof}

\subsection{Simplified Degree (1, 2, 2) Tetrahedral Update}

\subsection{Simplified Degree (1, 1, 1) and (2, 2, 2) Tetrahedral Updates}

\subsection{Midpoint Rule}

Like the righthand rule and ``modified midpoint'' rule, the midpoint
rule can be written in terms of a minimization of a line integral
involving a conic $Q$:
\begin{align*}
  \hat{u} = \min_{\lambda \in \Delta^2} F_1(\lambda) := \min_{\lambda \in \Delta^2} \curlyb{u_\lambda + \int_{[\hat{p}, p_\lambda]} sd\sigma} = \mbox{error} + \min_{\lambda \in \Delta^2} \curlyb{u_\lambda + h s_\lambda \sqrt{Q(\lambda)}},
\end{align*}
where $s_\lambda = s_0 + \lambda_1 \delta s_1 + \lambda_2 \delta s_2$.

\begin{lemma}
  The gradient and Hessian of $F_1$ with respect to $\lambda$ are given by:
  \begin{align*}
    \nabla_\lambda F_1(\lambda) &= \delta u + h \parens{l_\lambda \delta s + \frac{s_\lambda}{l_\lambda} \nabla_\lambda \parens{M\lambda + \epsilon}}, \\
    \nabla^2_\lambda F_1(\lambda) &= \frac{h}{l_\lambda} \parens{2\parens{\delta s - \frac{s_\lambda}{2Q(\lambda)}\parens{M\lambda + \epsilon}}\parens{M\lambda + \epsilon}^\top + s_\lambda M}.
  \end{align*}
\end{lemma}

\begin{proof}
  We have $\nabla_\lambda u_\lambda = \delta u$ and
  $\nabla_\lambda s_\lambda = \delta s$. Then:
  \begin{align*}
    \nabla_\lambda F_1(\lambda) = \delta u + h \parens{l_\lambda \delta s + \frac{s_\lambda}{2 l_\lambda} \nabla_\lambda Q(\lambda)},
  \end{align*}
  from which the form for $\nabla_\lambda F_1(\lambda)$ follows. Next,
  noting that:
  \begin{align*}
    \nabla_\lambda (l_\lambda \delta s) = \delta s \nabla_\lambda \sqrt{Q(\lambda)}^\top = \frac{1}{2l_\lambda} \delta s \nabla_\lambda Q(\lambda)^\top,
  \end{align*}
  and:
  \begin{align*}
    \nabla_\lambda \frac{s_\lambda}{l_\lambda} \nabla_\lambda Q(\lambda)
    &= \parens{\frac{1}{l_\lambda} \nabla_\lambda s_\lambda - \frac{s_\lambda}{2 Q(\lambda)^{3/2}} \nabla_\lambda Q(\lambda)} \nabla_\lambda Q(\lambda)^\top + \frac{s_\lambda}{l_\lambda} \nabla^2_\lambda Q(\lambda) \\
    &= \frac{1}{l_\lambda} \delta s \nabla_\lambda Q(\lambda)^\top - \frac{s_\lambda}{2 l_\lambda Q(\lambda)} \nabla_\lambda Q(\lambda) \nabla_\lambda Q(\lambda)^\top + \frac{s_\lambda}{l_\lambda} \nabla_\lambda^2 Q(\lambda),
  \end{align*}
  and simplifying using our forms for $\nabla_\lambda Q$ and
  $\nabla_\lambda^2 Q$ gives us our form for the Hessian.
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
